# Universal Logging System v2.0
## Complete Production-Grade Centralized Logging Platform

A comprehensive, enterprise-ready logging platform for collecting, storing, analyzing, and visualizing logs from multiple services, APIs, databases, and applications.

## ğŸš€ Features Overview

### Core Features
- **ğŸ” User Authentication** - JWT-based authentication with role-based access control (Admin, Developer, Viewer)
- **ğŸ“¨ Multi-Channel Alerts** - Email, Slack, and webhook notifications for critical logs
- **ğŸ“Š Real-time Analytics** - Live system health metrics, error trends, and performance monitoring
- **ğŸ” Advanced Search & Filters** - Search by service name, log level, server ID, error code, and keywords
- **ğŸ“¥ Log Export** - Export logs in CSV and JSON formats
- **ğŸ”‘ API Key Management** - Secure API keys for service authentication
- **âš¡ Real-time WebSocket** - Live log streaming via WebSocket connections
- **ğŸ“ˆ Visual Analytics** - Charts showing log distribution, error trends, and service health
- **ğŸ” Role-Based Access Control** - Admin, Developer, and Viewer roles with different permissions
- **ğŸ“‹ Log Retention Policies** - Automatic cleanup of old logs
- **ğŸŒ™ Dark Mode UI** - Developer-friendly, responsive interface

### Advanced Capabilities
- Batch log ingestion for high-volume environments
- Distributed tracing with trace ID support
- Request/Response correlation with request IDs
- Stack trace and error code tracking
- Service-level metrics and SLAs
- Alert deduplication to prevent notification spam
- Audit logging for compliance

## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (HTML5/CSS3)                     â”‚
â”‚         Login â†’ Dashboard â†’ Logs â†’ Analytics â†’ Alerts        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ REST API + WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Backend (Python)                   â”‚
â”‚  Auth â”‚ Logs â”‚ Analytics â”‚ Alerts â”‚ API Keys â”‚ Admin        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ ORM (SQLAlchemy)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Database (SQLite / PostgreSQL)                    â”‚
â”‚  Users â”‚ Logs â”‚ API Keys â”‚ Alert Rules â”‚ Alerts â”‚ Metrics   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Technology Stack

| Component | Technology |
|-----------|-----------|
| **Frontend** | HTML5, CSS3 (No frameworks) |
| **Backend** | FastAPI (Python) |
| **Database** | SQLite (dev) / PostgreSQL (prod) |
| **Authentication** | JWT Tokens |
| **Real-time** | WebSockets |
| **ORM** | SQLAlchemy |
| **Server** | Uvicorn |

## ğŸ“¦ Installation & Setup

### Prerequisites
- Python 3.8+
- PostgreSQL (optional, SQLite included)
- Modern web browser

### Step 1: Install Python Dependencies

```bash
cd backend
pip install -r requirements_enhanced.txt
```

### Step 2: Configure Environment

```bash
cp .env.example .env
# Edit .env with your settings
```

### Step 3: Initialize Database

```bash
python -c "from models import Base; from config import get_settings; from main import engine; Base.metadata.create_all(bind=engine)"
```

### Step 4: Start Backend Server

```bash
cd backend
python main.py
```

Server will start at: `http://localhost:8000`

**Default Admin Credentials:**
- Username: `admin`
- Password: `admin123`

### Step 5: Open Frontend

```bash
cd frontend
python -m http.server 8000
# In another terminal
# Open: http://localhost:8000/index_enhanced.html
```

## ğŸ“š API Documentation

### Authentication Endpoints

#### Register User
```http
POST /api/v1/auth/register
Content-Type: application/json

{
  "username": "developer1",
  "email": "dev@example.com",
  "password": "secure_password",
  "full_name": "Developer One"
}
```

#### Login
```http
POST /api/v1/auth/login
Content-Type: application/json

{
  "username": "admin",
  "password": "admin123"
}

Response:
{
  "access_token": "eyJhbGc...",
  "refresh_token": "eyJhbGc...",
  "token_type": "bearer"
}
```

#### Get Current User
```http
GET /api/v1/auth/me
Authorization: Bearer {access_token}
```

### Log Ingestion Endpoint

#### Ingest Logs
Service sends logs to this endpoint using API keys:

```http
POST /api/v1/logs/ingest?api_key=uls_xxxxx
Content-Type: application/json

[
  {
    "timestamp": "2026-02-09T10:30:00Z",
    "service_name": "user-service",
    "log_level": "ERROR",
    "message": "Database connection failed",
    "server_id": "server-01",
    "trace_id": "550e8400-e29b-41d4-a716-446655440000",
    "request_id": "req-12345",
    "user_id": "user-123",
    "error_code": "DB_TIMEOUT",
    "stack_trace": "at connectDB (db.js:123)",
    "response_time_ms": 5000.5
  }
]

Response:
{
  "status": "success",
  "message": "Ingested 1 logs",
  "count": 1
}
```

### Log Query Endpoints

#### Get All Logs
```http
GET /api/v1/logs?service_name=user-service&log_level=ERROR&limit=100&offset=0
Authorization: Bearer {access_token}
```

Query Parameters:
- `service_name` - Filter by service
- `log_level` - Filter by level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- `server_id` - Filter by server
- `trace_id` - Filter by trace ID
- `error_code` - Filter by error code
- `search` - Full-text search in message
- `start_time` - Filter logs after timestamp
- `end_time` - Filter logs before timestamp
- `limit` - Max results (default: 100, max: 10000)
- `offset` - Pagination offset (default: 0)

#### Get Log Statistics
```http
GET /api/v1/logs/stats?start_time=2026-02-09T00:00:00Z&end_time=2026-02-09T23:59:59Z
Authorization: Bearer {access_token}

Response:
{
  "total_logs": 5432,
  "error_count": 234,
  "warning_count": 567,
  "info_count": 3456,
  "debug_count": 1175,
  "critical_count": 0,
  "services": ["user-service", "auth-service", "api-gateway"],
  "error_rate": 4.31,
  "avg_response_time_ms": 125.5
}
```

#### Get Log Detail
```http
GET /api/v1/logs/{log_id}
Authorization: Bearer {access_token}
```

### Export Endpoints

#### Export to CSV
```http
POST /api/v1/logs/export/csv?service_name=user-service&log_level=ERROR
Authorization: Bearer {access_token}

# Returns CSV file with columns:
# ID, Timestamp, Service, Level, Message, Server, TraceID, ErrorCode, ResponseTime
```

#### Export to JSON
```http
POST /api/v1/logs/export/json?service_name=user-service
Authorization: Bearer {access_token}

Response:
{
  "logs": [
    {
      "id": 1,
      "timestamp": "2026-02-09T10:30:00Z",
      "service_name": "user-service",
      "log_level": "ERROR",
      "message": "Database error",
      ...
    }
  ],
  "count": 100
}
```

### Alert Management Endpoints

#### Create Alert Rule
```http
POST /api/v1/alerts/rules
Authorization: Bearer {access_token}
Content-Type: application/json

{
  "name": "High Error Rate",
  "description": "Alert when error rate exceeds threshold",
  "service_name": "user-service",
  "log_level": "ERROR",
  "error_code": null,
  "keyword_match": null,
  "threshold": 5,
  "time_window_seconds": 300,
  "alert_type": "email",
  "alert_target": "admin@example.com"
}
```

#### Get Alert Rules
```http
GET /api/v1/alerts/rules
Authorization: Bearer {access_token}
```

#### Get Triggered Alerts
```http
GET /api/v1/alerts?is_resolved=false
Authorization: Bearer {access_token}
```

### API Key Management

#### Create API Key
```http
POST /api/v1/api-keys?name=Production+Key&service_name=user-service
Authorization: Bearer {access_token}

Response:
{
  "key": "uls_a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6",
  "name": "Production Key",
  "service_name": "user-service",
  "created_at": "2026-02-09T10:30:00Z"
}
```

#### List API Keys
```http
GET /api/v1/api-keys
Authorization: Bearer {access_token}
```

## ğŸ”Œ Integration Examples

### Node.js / JavaScript
```javascript
const apiKey = 'uls_xxxxx';

async function sendLogs(logs) {
  const response = await fetch(
    `http://logging-system.com/api/v1/logs/ingest?api_key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(logs)
    }
  );
  return response.json();
}

// Send error log
sendLogs([{
  service_name: 'user-service',
  log_level: 'ERROR',
  message: 'Failed to fetch user',
  trace_id: crypto.randomUUID(),
  server_id: 'server-01'
}]);
```

### Python
```python
import requests
import json
from datetime import datetime
import uuid

API_KEY = "uls_xxxxx"
BASE_URL = "http://localhost:8000/api/v1"

def send_logs(logs):
    url = f"{BASE_URL}/logs/ingest?api_key={API_KEY}"
    response = requests.post(url, json=logs)
    return response.json()

# Send error log
logs = [{
    "timestamp": datetime.utcnow().isoformat() + "Z",
    "service_name": "user-service",
    "log_level": "ERROR",
    "message": "Database connection failed",
    "server_id": "server-01",
    "trace_id": str(uuid.uuid4()),
    "error_code": "DB_TIMEOUT",
    "response_time_ms": 5000.0
}]

response = send_logs(logs)
print(response)
```

### Java
```java
import java.net.HttpURLConnection;
import java.net.URL;
import com.google.gson.Gson;
import com.google.gson.JsonArray;
import com.google.gson.JsonObject;

public class LogWriter {
    private static final String API_KEY = "uls_xxxxx";
    private static final String BASE_URL = "http://localhost:8000/api/v1";
    
    public static void sendLog(String service, String level, String message) throws Exception {
        JsonObject log = new JsonObject();
        log.addProperty("timestamp", new java.util.Date().toString());
        log.addProperty("service_name", service);
        log.addProperty("log_level", level);
        log.addProperty("message", message);
        
        JsonArray logs = new JsonArray();
        logs.add(log);
        
        URL url = new URL(BASE_URL + "/logs/ingest?api_key=" + API_KEY);
        HttpURLConnection conn = (HttpURLConnection) url.openConnection();
        conn.setRequestMethod("POST");
        conn.setRequestProperty("Content-Type", "application/json");
        conn.setDoOutput(true);
        
        try (java.io.OutputStream os = conn.getOutputStream()) {
            os.write(logs.toString().getBytes());
        }
        
        int status = conn.getResponseCode();
        System.out.println("Status: " + status);
    }
}
```

### cURL
```bash
# Create API Key
curl -X POST "http://localhost:8000/api/v1/api-keys?name=My%20Key&service_name=my-service" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  | jq '.key'

# Send Logs
curl -X POST "http://localhost:8000/api/v1/logs/ingest?api_key=uls_xxxxx" \
  -H "Content-Type: application/json" \
  -d '[{
    "service_name": "my-service",
    "log_level": "ERROR",
    "message": "Something went wrong",
    "server_id": "server-01",
    "trace_id": "550e8400-e29b-41d4-a716-446655440000"
  }]'

# Query Logs
curl "http://localhost:8000/api/v1/logs?log_level=ERROR&limit=10" \
  -H "Authorization: Bearer YOUR_TOKEN"

# Get Statistics
curl "http://localhost:8000/api/v1/logs/stats" \
  -H "Authorization: Bearer YOUR_TOKEN" | jq
```

## ğŸ¯ Dashboard Features

### Dashboard Tab
- **Statistics Cards** - Total logs, errors, warnings, info messages
- **System Health Indicator** - Real-time health percentage
- **Performance Metrics** - Average response time, error rate
- **Recent Logs** - Last 10 log entries in real-time

### Logs Tab
- **Advanced Filters** - Service, level, time range
- **Full-Text Search** - Search across log messages
- **Export Options** - Download as CSV or JSON
- **Detailed View** - Click "View" to see complete log entry

### Analytics Tab
- **Log Level Distribution** - Bar chart of log counts by level
- **Services Overview** - List of active services
- **Error Trends** - 24-hour error trend visualization

### Alerts Tab
- **Create New Rules** - Set up automated alerting
- **Alert Rules List** - View all configured rules
- **Email/Slack/Webhook** - Multiple notification channels
- **Threshold-based** - Trigger after N occurrences

### API Keys Tab
- **Generate Keys** - Create secure API keys for services
- **Key Management** - View and manage active keys
- **Copy to Clipboard** - Easy key duplication
- **Service Tracking** - See which service uses which key

### Settings Tab
- **User Preferences** - Email notifications, dark mode
- **Auto-refresh** - Configure log refresh interval
- **Data Management** - Clear old logs

## ğŸ”’ Security Features

1. **JWT Authentication** - Secure token-based authentication
2. **API Keys** - Service-to-API authentication with unique keys
3. **Role-Based Access Control** - Admin, Developer, Viewer roles
4. **Password Hashing** - Bcrypt password hashing
5. **CORS Protection** - Configurable cross-origin access
6. **Input Validation** - All inputs validated for safety
7. **SQL Injection Prevention** - SQLAlchemy ORM prevents injection
8. **XSS Protection** - HTML escaping in frontend

## ğŸ“Š Scaling Considerations

### For High-Volume Logs
1. Use PostgreSQL instead of SQLite
2. Add database indexes on frequently filtered columns
3. Implement log sharding by service or time
4. Use message queue (Redis/Kafka) for async processing
5. Archive old logs to S3/archival storage
6. Implement caching for frequently accessed data

### Database Schema Optimization
```sql
-- Create indexes for fast queries
CREATE INDEX idx_timestamp_service ON logs(timestamp, service_name);
CREATE INDEX idx_timestamp_level ON logs(timestamp, log_level);
CREATE INDEX idx_service_level ON logs(service_name, log_level);
CREATE INDEX idx_trace_id ON logs(trace_id);
```

### Performance Tips
- Batch log ingestion (100-1000 logs per request)
- Use time-range filters for queries
- Implement log rotation/archival
- Monitor database disk space
- Regular index maintenance

## ğŸš€ Deployment

### Docker Deployment
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY backend/requirements_enhanced.txt .
RUN pip install -r requirements_enhanced.txt

COPY backend /app

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose
```yaml
version: '3.8'
services:
  api:
    image: logging-system:latest
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://user:password@db:5432/logs
    depends_on:
      - db
  
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: logs
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### Cloud Deployment
- **Heroku**: Push code directly
- **AWS**: ECS, Lambda, RDS
- **GCP**: Cloud Run, Cloud SQL
- **Azure**: App Service, SQL Database
- **DigitalOcean**: App Platform, Managed Databases

## ğŸ› Troubleshooting

### API Connection Issues
```
Error: Connection refused to localhost:8000
Solution: Ensure backend is running with `python main.py`
```

### Database Errors
```
Error: database is locked
Solution: Close other connections or use PostgreSQL
```

### CORS Errors
```
Error: No 'Access-Control-Allow-Origin' header
Solution: Check CORS_ORIGINS in .env file
```

### WebSocket Connection Fails
```
Error: WebSocket connection failed
Solution: Check firewall, ensure /ws/logs endpoint is accessible
```

## ğŸ“ˆ Performance Benchmarks

- **Log Ingestion**: 10,000+ logs/second (PostgreSQL)
- **Query Response**: <200ms for filtered queries (with indexes)
- **Dashboard Load**: <500ms (with caching)
- **Real-time Updates**: <100ms latency via WebSocket

## ğŸ“ Log Fields Reference

| Field | Type | Description |
|-------|------|-------------|
| timestamp | DateTime | When log was created |
| service_name | String | Source service name |
| log_level | String | DEBUG, INFO, WARNING, ERROR, CRITICAL |
| message | Text | Log message |
| server_id | String | Server/instance identifier |
| trace_id | String | Distributed trace ID |
| request_id | String | HTTP request/operation ID |
| user_id | String | Affected user ID |
| error_code | String | Application error code |
| stack_trace | Text | Stack trace for errors |
| metadata | JSON | Custom metadata |
| response_time_ms | Float | Operation duration |

## ğŸ”— Further Resources

- [Full API Documentation](API_DOCUMENTATION.md)
- [Configuration Guide](CONFIGURATION.md)
- [Deployment Guide](DEPLOYMENT.md)

## ğŸ“ Support

For issues and questions:
1. Check the FAQ section below
2. Review logs in `/var/log/logging-system/`
3. Check database connectivity
4. Verify API credentials

## â“ FAQs

**Q: Can I use SQLite in production?**
A: SQLite is best for < 1000 logs/day. Use PostgreSQL for production.

**Q: How long are logs retained?**
A: Default 30 days. Configure in `.env` with `LOG_RETENTION_DAYS`.

**Q: How do I reset the admin password?**
A: Delete `logging_system.db` and restart - default user recreates.

**Q: Can I integrate with my existing ELK stack?**
A: Yes, implement a custom exporter to send logs to Elasticsearch.

**Q: What's the maximum log size?**
A: Message field supports up to 64MB in PostgreSQL.

---

**Universal Logging System v2.0** - Enterprise-Grade Logging Platform
Built with FastAPI, SQLAlchemy, and modern web technologies
